# è¥¿ç“œä¹¦ä»£ç å®æˆ˜

> æœ¬æ•™ç¨‹ä¸ºå‘¨å¿—åè€å¸ˆçš„ã€Šæœºå™¨å­¦ä¹ ã€‹ï¼ˆç®€ç§°â€œè¥¿ç“œä¹¦â€ï¼‰ğŸ‰é…å¥—ä»£ç å®æˆ˜æ•™ç¨‹ï¼ŒGitHubé“¾æ¥ï¼šhttps://github.com/datawhalechina/machine-learning-toy-code
>
> ã€ä½œè€…ç®€ä»‹ã€‘
>
> ç‰§å°ç†Šï¼ŒDatawhaleæˆå‘˜&æ­¦æ±‰åˆ†éƒ¨è´Ÿè´£äººï¼Œç®—æ³•å·¥ç¨‹å¸ˆ
>
> çŸ¥ä¹ä¸»é¡µï¼šhttps://www.zhihu.com/people/muxiaoxiong
>
> ã€ç›¸å…³æ¨èã€‘
>
> è¥¿ç“œä¹¦é…å¥—å…¬å¼æ¨å¯¼æ•™ç¨‹â€”â€”ğŸƒå—ç“œä¹¦ï¼šhttps://github.com/datawhalechina/pumpkin-book

# ç¬¬1ç« ç»ªè®º

Sklearnï¼ˆScikit-Learnï¼‰æ˜¯ä¸€ä¸ªåŸºäºPythonè¯­è¨€çš„å¼ºå¤§çš„æœºå™¨å­¦ä¹ å·¥å…·åº“ï¼Œå®ƒå»ºç«‹åœ¨NumPyã€SciPyã€Pandaså’ŒMatplotlibç­‰ç§‘å­¦è®¡ç®—åº“ä¹‹ä¸Šï¼Œæä¾›äº†ä¸€æ•´å¥—æœºå™¨å­¦ä¹ ç®—æ³•å’Œæ•°æ®é¢„å¤„ç†åŠŸèƒ½ã€‚

Sklearn å› å…¶æ˜“ç”¨æ€§ã€ç»Ÿä¸€è€Œä¼˜é›…çš„APIè®¾è®¡ã€ä¸°å¯Œçš„ç®—æ³•æ”¯æŒä»¥åŠæ´»è·ƒçš„ç¤¾åŒºï¼Œæˆä¸ºè®¸å¤šæœºå™¨å­¦ä¹ å¼€å‘è€…å’Œç ”ç©¶äººå‘˜çš„é¦–é€‰å·¥å…·ã€‚å…·ä½“å¦‚ä¸‹ï¼š

1. **ä¸»è¦åŠŸèƒ½**ï¼šSklearnåŒ…å«äº†å…­å¤§ä»»åŠ¡æ¨¡å—ï¼šåˆ†ç±»ã€å›å½’ã€èšç±»ã€é™ç»´ã€æ¨¡å‹é€‰æ‹©å’Œé¢„å¤„ç†ã€‚è¿™äº›æ¨¡å—æ¶µç›–äº†æœºå™¨å­¦ä¹ å®éªŒçš„ä¸»è¦æ­¥éª¤ï¼Œä»æ•°æ®é¢„å¤„ç†åˆ°æ¨¡å‹çš„é€‰æ‹©å’Œè¯„ä¼°ã€‚
2. **ç®—æ³•æ”¯æŒ**ï¼šSklearnæä¾›äº†ä¸°å¯Œçš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼ŒåŒ…æ‹¬çº¿æ€§æ¨¡å‹ï¼ˆå¦‚çº¿æ€§å›å½’ï¼‰ã€å†³ç­–æ ‘ã€æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ã€éšæœºæ£®æ—ã€K-meansèšç±»å’Œä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ç­‰ã€‚è¿™äº›ç®—æ³•åœ¨ç›‘ç£å­¦ä¹ ï¼ˆåˆ†ç±»å’Œå›å½’ï¼‰ã€æ— ç›‘ç£å­¦ä¹ ï¼ˆèšç±»å’Œé™ç»´ï¼‰ä¸­éƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚
3. **ç‰¹ç‚¹ä¼˜åŠ¿**ï¼šSklearnçš„è®¾è®¡å¼ºè°ƒä¸€è‡´æ€§ã€å¯æ£€éªŒã€æ ‡å‡†ç±»ã€å¯ç»„åˆå’Œé»˜è®¤å€¼ç­‰åŸåˆ™ã€‚è¿™æ„å‘³ç€ä¸åŒåŠŸèƒ½çš„APIæœ‰ç»Ÿä¸€çš„è°ƒç”¨æ–¹å¼ï¼Œä¾¿äºç”¨æˆ·å¿«é€Ÿä¸Šæ‰‹å¹¶çµæ´»åº”ç”¨ã€‚
4. **æ•°æ®å¤„ç†**ï¼šåœ¨æ•°æ®é¢„å¤„ç†æ–¹é¢ï¼ŒSklearnå…·å¤‡å¼ºå¤§çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ•°æ®æ¸…æ´—ã€æ ‡å‡†åŒ–ã€ç‰¹å¾ç¼–ç ã€ç‰¹å¾æå–ç­‰ã€‚è¿™ä½¿å¾—ä»åŸå§‹æ•°æ®åˆ°å¯ç”¨äºæ¨¡å‹è®­ç»ƒçš„æ ¼å¼è½¬æ¢å˜å¾—ç®€å•é«˜æ•ˆã€‚
5. **æ¨¡å‹è¯„ä¼°**ï¼šä¸ºäº†å¸®åŠ©ç”¨æˆ·è¯„ä¼°å’Œä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼ŒSklearnæä¾›äº†å¤šç§å·¥å…·ï¼ŒåŒ…æ‹¬äº¤å‰éªŒè¯ã€ç½‘æ ¼æœç´¢ä»¥åŠå„ç§æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°ï¼‰ã€‚
6. **ç¤¾åŒºæ›´æ–°**ï¼šSklearnæ‹¥æœ‰æ´»è·ƒçš„ç¤¾åŒºæ”¯æŒï¼Œå¹¶ä¸”æŒç»­æ›´æ–°ã€‚æœ€æ–°çš„ç‰ˆæœ¬è¦æ±‚ä½¿ç”¨Python 3.6æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œä¿æŒä¸å½“å‰æŠ€æœ¯ç¯å¢ƒçš„å…¼å®¹æ€§ã€‚

sklearnå®‰è£…

```Plain
pip install -U scikit-learn

conda install scikit-learn
```

**å…³è”é“¾æ¥ï¼š**

https://scikit-learn.org/stable/index.html

https://www.scikitlearn.com.cn/

# ç¬¬2ç« æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©

## 2.2.1ç•™å‡ºæ³•

> â€œç•™å‡ºæ³•â€ç›´æ¥å°†æ•°æ®é›†Dåˆ’åˆ†ä¸ºä¸¤ä¸ªäº’æ–¥çš„é›†åˆï¼Œå…¶ä¸­ä¸€ä¸ªé›†åˆä½œä¸ºè®­ç»ƒé›†ï¼Œå¦ä¸€ä¸ªä½œä¸ºæµ‹è¯•é›†

train_test_splitæ–¹æ³•èƒ½å¤Ÿå°†æ•°æ®é›†æŒ‰ç…§ç”¨æˆ·çš„éœ€è¦æŒ‡å®šåˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†

| train_data   | å¾…åˆ’åˆ†çš„æ ·æœ¬ç‰¹å¾é›†åˆ                                         |
| ------------ | ------------------------------------------------------------ |
| X_train      | åˆ’åˆ†å‡ºçš„è®­ç»ƒæ•°æ®é›†æ•°æ®                                       |
| X_test       | åˆ’åˆ†å‡ºçš„æµ‹è¯•æ•°æ®é›†æ•°æ®                                       |
| y_train      | åˆ’åˆ†å‡ºçš„è®­ç»ƒæ•°æ®é›†çš„æ ‡ç­¾                                     |
| y_test       | åˆ’åˆ†å‡ºçš„æµ‹è¯•æ•°æ®é›†çš„æ ‡ç­¾                                     |
| test_size    | è‹¥åœ¨0~1ä¹‹é—´ï¼Œä¸ºæµ‹è¯•é›†æ ·æœ¬æ•°ç›®ä¸åŸå§‹æ ·æœ¬æ•°ç›®ä¹‹æ¯”ï¼›è‹¥ä¸ºæ•´æ•°ï¼Œåˆ™æ˜¯æµ‹è¯•é›†æ ·æœ¬çš„æ•°ç›® |
| random_state | éšæœºæ•°ç§å­ï¼Œä¸åŒçš„éšæœºæ•°ç§å­åˆ’åˆ†çš„ç»“æœä¸åŒ                   |
| stratify     | stratifyæ˜¯ä¸ºäº†ä¿æŒsplitå‰ç±»çš„åˆ†å¸ƒï¼Œä¾‹å¦‚è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ•°é‡çš„æ¯”ä¾‹æ˜¯ Aï¼šB= 4ï¼š1ï¼Œç­‰åŒäºsplitå‰çš„æ¯”ä¾‹ï¼ˆ80ï¼š20ï¼‰ã€‚é€šå¸¸åœ¨è¿™ç§ç±»åˆ†å¸ƒä¸å¹³è¡¡çš„æƒ…å†µä¸‹ä¼šç”¨åˆ°stratifyã€‚ |

å‡è®¾æˆ‘ä»¬ç°åœ¨æœ‰æ•°æ®é›†Dï¼Œå°†æ•°æ®é›†æŒ‰ç…§0.8:0.2çš„æ¯”ä¾‹åˆ’åˆ†ä¸ºæ•°æ®é›†ä¸æµ‹è¯•é›†

```Python
from sklearn.datasets import load_iris

# åŠ è½½æ•°æ®é›†
iris = load_iris()
X = iris.data
y = iris.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

## 2.2.2 äº¤å‰éªŒè¯æ³•

> è¦åœ¨sklearnä¸­å®ç°äº¤å‰éªŒè¯ï¼Œå¯ä»¥ä½¿ç”¨KFoldå‡½æ•°

```Python
from sklearn.model_selection import KFold
from sklearn.datasets import load_iris

# åŠ è½½æ•°æ®é›†
iris = load_iris()
X, y = iris.data, iris.target

# å®šä¹‰æ¨¡å‹
# å‡è®¾ç°åœ¨æœ‰ä¸€ä¸ªæ¨¡å‹model
model 

# å®šä¹‰KæŠ˜äº¤å‰éªŒè¯
kf = KFold(n_splits=5)

# è¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(X_train, y_train)
    
    # é¢„æµ‹æµ‹è¯•é›†
    y_pred = model.predict(X_test)
    
```

## 2.2.3è‡ªåŠ©æ³•

```Python
import numpy as np
from sklearn.utils import resample

#å‡è®¾æœ‰ä¸€ä¸ªæ•°æ®é›†Xå’Œå¯¹åº”çš„æ ‡ç­¾y
X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])
y = np.array([0, 1, 0, 1, 0])

#ä½¿ç”¨è‡ªåŠ©æ³•è¿›è¡ŒæŠ½æ ·
bootstrap_samples = []
for _ in range(100):  # ç”Ÿæˆ10ä¸ªè‡ªåŠ©æ ·æœ¬
    X_resampled, y_resampled = resample(X, y)
    bootstrap_samples.append((X_resampled, y_resampled))
    
#bootstrap_samplesç°åœ¨åŒ…å«äº†100ä¸ªè‡ªåŠ©æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æ˜¯åŸå§‹æ•°æ®é›†çš„ä¸€ä¸ªéšæœºå­é›†
```

## 2.3æ€§èƒ½åº¦é‡

### å‡æ–¹è¯¯å·®

$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

```Python
#ä½¿ç”¨sklearnåŒ…å®ç°
from sklearn.metrics import mean_squared_error

y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]

mse = mean_squared_error(y_true, y_pred)
print("MSE:", mse)
```

### é”™è¯¯ç‡

é”™è¯¯ç‡å…¬å¼ï¼š  

$$E = \frac{e}{m} $$  

å…¶ä¸­ï¼š
- $e$ï¼šåˆ†ç±»é”™è¯¯çš„æ ·æœ¬æ•°é‡ï¼›
- $m$ï¼šæ ·æœ¬æ€»æ•°ã€‚

```Python
def error_rate(y_pred, y_true):
    """
    è®¡ç®—é”™è¯¯ç‡çš„å‡½æ•°ã€‚
    
    å‚æ•°ï¼š
    predicted (list): é¢„æµ‹å€¼åˆ—è¡¨
    actual (list): å®é™…å€¼åˆ—è¡¨
    
    è¿”å›ï¼š
    float: é”™è¯¯ç‡
    """
    total = len(y_pred)
    errors = sum(1 for p, a in zip(y_pred, y_true) if p != a)
    error_rate = errors / total
    return error_rate

# ç¤ºä¾‹
y_true = [1, 0, 1, 1, 0]
y_pred = [1, 1, 0, 1, 0]
error_rate = error_rate(y_pred , y_true )
print("é”™è¯¯ç‡ï¼š", error_rate)

#é”™è¯¯ç‡å¯ä»¥ç”¨1-ç²¾åº¦æ¥è·å¾—
```

### ç²¾åº¦

$$
\text{ç²¾åº¦} = 1 - \frac{a}{m}
$$  
å…¶ä¸­ï¼š
- $a$ï¼šåˆ†ç±»æ­£ç¡®çš„æ ·æœ¬æ•°ï¼›
- $m$ï¼šæ ·æœ¬æ€»æ•°

ç²¾åº¦ä¸é”™è¯¯ç‡ï¼ˆError Rateï¼‰äº’ä¸ºè¡¥é›†ï¼Œæ»¡è¶³ï¼š
$$ç²¾åº¦=1âˆ’é”™è¯¯ç‡$$

```Python
#ä½¿ç”¨sklearn è®¡ç®—ç²¾åº¦
from sklearn.metrics import accuracy_score

# å‡è®¾y_trueæ˜¯å®é™…æ ‡ç­¾ï¼Œy_predæ˜¯é¢„æµ‹æ ‡ç­¾
y_true = [1, 0, 1, 1, 0]
y_pred = [1, 1, 0, 1, 0]

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(y_true, y_pred)
print("ç²¾åº¦ï¼š", accuracy)
```

### æŸ¥å‡†ç‡

$${precision=\frac{{TP}}{{TP+FP}}}$$

```Python
##########
from sklearn.metrics import precision_score

# å‡è®¾y_trueæ˜¯çœŸå®çš„æ ‡ç­¾åˆ—è¡¨ï¼Œy_predæ˜¯é¢„æµ‹çš„æ ‡ç­¾åˆ—è¡¨
y_true = [1, 0, 1, 1, 0, 1]
y_pred = [1, 1, 0, 1, 0, 1]

# è®¡ç®—ç²¾ç¡®ç‡
precision = precision_score(y_true, y_pred)

print("ç²¾ç¡®ç‡ï¼š", precision)
```

### æŸ¥å…¨ç‡

$${Recall=\frac{{TP}}{{TP+FN}}}$$

```Python
###################
from sklearn.metrics import recall_score

# å‡è®¾y_trueæ˜¯çœŸå®çš„æ ‡ç­¾åˆ—è¡¨ï¼Œy_predæ˜¯é¢„æµ‹çš„æ ‡ç­¾åˆ—è¡¨
y_true = [1, 0, 1, 1, 0, 1]
y_pred = [1, 1, 0, 1, 0, 1]

# è®¡ç®—å¬å›ç‡
recall = recall_score(y_true, y_pred)

print("å¬å›ç‡ï¼š", recall)
```

### F1å€¼

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=MGJmOWY3NGEyYzQwOWJjMTE5ZTJkMDNhZjBkMjY2MGFfNGV3RDlITkFTZHNHdkdwRDZlUVh0S2hOSnBWZkVZVHFfVG9rZW46UzlmVmJXb25Ub2MxQnJ4S2VYemNUVEJnbk1nXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

```Python
#ä½¿ç”¨sklearn è®¡ç®—F1å€¼
from sklearn.metrics import f1_score
y_true = [1, 0, 1, 1, 0]
y_pred = [1, 1, 0, 1, 0]
f1 = f1_score(y_true, y_pred)
print("F1å€¼ï¼š", f1)
```

### AUCå€¼

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=N2UyMjNjZDgyOTI1YzRhNTRkOTdiNjg1ZWM5ODhjMTBfSG5DcGpmWktKOU9kSEcwWk5ydjNVV2xINWZwZXl6SFhfVG9rZW46VVZKVGJtZkttb0VRSzZ4NTV2U2MzRjlkblRlXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

```Python
#ä½¿ç”¨sklearn è®¡ç®—AUCå€¼
from sklearn.metrics import roc_auc_score

# å‡è®¾y_trueæ˜¯çœŸå®æ ‡ç­¾ï¼Œy_predæ˜¯é¢„æµ‹æ¦‚ç‡
from sklearn.metrics import roc_auc_score
y_true = [1, 0, 1, 1, 0]
y_pred = [0.9, 0.8, 0.7, 0.6, 0.5]

# è®¡ç®—AUCå€¼
auc = roc_auc_score(y_true, y_pred)
print("AUCå€¼ä¸ºï¼š", auc)
```

## 2.5åå·®ä¸æ–¹å·®

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=ZDZiMjUxOTFlMzBlOWU0ZWVkMGMyNDM1ZmQ0MDA5YWJfSU82MFVMa0tIUThwWm9XcmhhbVlGckxMbWRyWnRXRzRfVG9rZW46U0UyTGJzNVFYb3liWmJ4QXNYSWNXTWgzbjhnXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

```Python
import math
y_true = [1, 0, 1, 1, 0]
y_pred = [0.9, 0.8, 0.7, 0.6, 0.5]

bias_2=sum([(x - y) ** 2 for x, y in zip(y_true , y_pred )])
bias=math.sqrt(bias_2)
```

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=NDYzY2MzM2UzODhlMjY2YmIzNGQ5YTFiZDU5ZjEzZWFfbVhONnREZmhqbmlGQTdNZGpiQzhIdlhkbzRGSzVPdGxfVG9rZW46UDN0NmJzS09Eb1RFeXh4eTRzNmM0d3FablhRXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

```Python
from sklearn.metrics import variance_score
y_true = [1, 0, 1, 1, 0]
y_pred = [0.9, 0.8, 0.7, 0.6, 0.5]

# è®¡ç®—æ–¹å·®
var= variance_score(y_true, y_pred)
print("æ–¹å·®ï¼š", var)
```

# ç¬¬3ç« çº¿æ€§æ¨¡å‹

æœ¬èŠ‚ä¸»è¦å®ç°çº¿æ€§æ¨¡å‹ä¸­çš„çº¿æ€§å›å½’ä¸é€»è¾‘å›å½’

## çº¿æ€§å›å½’

çº¿æ€§å›å½’æ˜¯æ¯”è¾ƒç®€å•çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œé€šå¸¸ä½œä¸ºæœºå™¨å­¦ä¹ å…¥é—¨ç¬¬ä¸€è¯¾ã€‚

çº¿æ€§å›å½’çš„ä¸€èˆ¬å½¢å¼ä¸º

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=NGI0ZWJmNDJhMzVjYzcxOGU5MmVkMGYwZmZiMmM0MTFfRDRBWWNQNkZSc3IyUFpRYkFzWjJGeXlsbXM4ckRJSWhfVG9rZW46TnVqSGI1VDBnbzlsZll4WUNlcGNrdlhjbmZnXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjFmNGExYjQ1NmUzMWRmOGM5Nzg4MDU2ZDlhNzllYmNfNzZQaW1yN1VBRUU2dlhKREMwVzhXRGZ3MjVoV3RNMnpfVG9rZW46R1ZYU2JDbmNNb0h3bnJ4M2pYWmNCUWwybndiXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

é€šå¸¸æƒ…å†µä¸‹ç³»æ•°wå’Œb æ— æ³•ä¹‹é—´æ±‚è§£ï¼Œæˆ‘ä»¬å¾€å¾€ä¼šç”¨æ¢¯åº¦ä¸‹é™æ³•è¿›è¡Œè§£å†³ï¼Œå…·ä½“è€Œè¨€ï¼Œå°±æ˜¯ç»™w å’Œ b ä¸€ä¸ªåˆå§‹å€¼ï¼Œè®¡ç®—å‡æ–¹è¯¯å·®çš„æ¢¯åº¦ï¼Œä»è€Œç»§ç»­æ›´æ–°å‚æ•°ï¼Œäºæ˜¯å¯¹ä¸Šé¢çš„3.4çš„å…¬å¼æ±‚åå¯¼å¯ä»¥å¾—åˆ°

â€‹                                     $${\frac{{ \partial loss}}{{ \partial w}}={\mathop{ \sum }\limits_{{i=1}}^{{m}}{2 \left( wx\mathop{{}}\nolimits_{{i}}+b-y\mathop{{}}\nolimits_{{i}} \left) x\mathop{{}}\nolimits_{{i}}\right. \right. }}}$$

â€‹                                     $$ {\frac{{ \partial loss}}{{ \partial b}}={\mathop{ \sum }\limits_{{i=1}}^{{m}}{2 \left( wx\mathop{{}}\nolimits_{{i}}+b-y\mathop{{}}\nolimits_{{i}} \right) }}}$$                       

### æ„é€ æ•°æ®é›†

```Python
import numpy as np 

#ç”Ÿæˆä¸€ä¸ªæ ·æœ¬é‡ä¸º100 ç‰¹å¾é‡ä¸º10çš„æ•°æ®é›†
sample=100
feature=10
X=np.random.rand(sample,10)
#æ„é€ ç›®æ ‡å€¼ y=3+2*x1+5*x2-3*x3
y=3+2*X[:,0]+5*X[:,1]-3*X[:,2]+np.random.randn(sample) # å¢åŠ ä¸€ç‚¹å™ªéŸ³
#np.random.randn ç”Ÿæˆæ»¡è¶³æ­£æ€åˆ†å¸ƒçš„éšæœºæ•°
```

### æ„å»ºæ¨¡å‹

```Python
class LR():
    def __init__(self):
        self.w=None
        self.b=None 
        
    def get_loss(self,y,y_pre):
        """
        å®šä¹‰æŸå¤±å‡½æ•°f(x)--> å‡æ–¹è¯¯å·®
        y: çœŸå®å€¼
        y_pre: é¢„æµ‹å€¼
        """
        loss=np.mean((y-y_pre)**2)
        return loss
    
    def fit(self,x,y,learning_rate=0.01,n_iterations=500):
        """
        x è®­ç»ƒæ•°æ®
        y ç›®æ ‡å€¼
        learning_rate å­¦ä¹ ç‡
        n_iterations è¿­ä»£æ¬¡æ•°
        """
        sample,feature=x.shape
        
        if self.w==None:
            #åˆå§‹åŒ–æ•°æ®
            self.w=np.random.randn(feature)
            self.b=0
            
        #å¼€å§‹è®­ç»ƒ
        for i in range(n_iterations):
            y_pre=np.dot(x,self.w)+self.b
            #è®¡ç®—é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹å·®  
            # !!æ³¨æ„è¿™é‡Œæ˜¯é¢„æµ‹å€¼å‡çœŸå®å€¼
            diff=y_pre-y
            #è®¡ç®—æŸå¤±å‡½æ•°çš„æ¢¯åº¦
            dw=2/sample*np.dot(x.T,diff)
            db=2/sample*np.sum(diff,axis=0)
            
            #æ›´æ–°å‚æ•°
            self.w=self.w-learning_rate*dw
            self.b=self.b-learning_rate*db
            
            #è®¡ç®—loss 
            loss=self.get_loss(y,y_pre)
            print("epoch:{}  loss:{}".format(i,loss))
                  
    def predict(self,x):
        y_pre=np.dot(x,self.w)+self.b
        return y_pre
```

### é¢„æµ‹ç»“æœ

```Python
#ç”Ÿæˆä¸€ä¸ªæ ·æœ¬é‡ä¸º100 ç‰¹å¾é‡ä¸º10çš„æ•°æ®é›†
sample=1
feature=10
test_x=np.random.rand(sample,10)
#æ„é€ ç›®æ ‡å€¼ y=3+2*x1+5*x2-3*x3
test_y=3+2*test_x[:,0]+5*test_x[:,1]-3*test_x[:,2]+np.random.randn(sample) # å¢åŠ ä¸€ç‚¹å™ªéŸ³
#np.random.randn ç”Ÿæˆæ»¡è¶³æ­£æ€åˆ†å¸ƒçš„éšæœºæ•°

model=LR()
model.fit(X,y)
y_pre=model.predict(test_x)
```

### ä½¿ç”¨sklearnåŒ…å®ç°

```Python
#æ„é€ æ•°æ®é›†
import numpy as np 

#ç”Ÿæˆä¸€ä¸ªæ ·æœ¬é‡ä¸º100 ç‰¹å¾é‡ä¸º10çš„æ•°æ®é›†
sample=100
feature=10
X=np.random.rand(sample,feature)
#æ„é€ ç›®æ ‡å€¼ y=3+2*x1+5*x2-3*x3
y=3+2*X[:,0]+5*X[:,1]-3*X[:,2]+np.random.randn(sample) # å¢åŠ ä¸€ç‚¹å™ªéŸ³
#np.random.randn ç”Ÿæˆæ»¡è¶³æ­£æ€åˆ†å¸ƒçš„éšæœºæ•°

from sklearn.linear_model import LinearRegression # çº¿æ€§å›å½’æ¨¡å‹
from sklearn.metrics import mean_squared_error # è¯„ä»·æŒ‡æ ‡

model=LinearRegression() 
model.fit(X,y)
y_pre=model.predict(test_x)

#æ˜¾ç¤ºæƒé‡ç³»æ•°
weights = model.coef_
intercept = model.intercept_

print("æƒé‡ï¼š", weights)
print("åç§»ï¼š", intercept)
```

**å‚æ•°è®²è§£**

fit() å‡½æ•°ä¸­å¸¦çš„å‚æ•°

- fit_intercept: æ˜¯å¦è®¡ç®—æˆªè·ã€‚é»˜è®¤ä¸º Trueï¼Œè¡¨ç¤ºè®¡ç®—æˆªè·ã€‚
- normalize: æ˜¯å¦åœ¨å›å½’å‰å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ã€‚é»˜è®¤ä¸º Falseã€‚
- copy_X: æ˜¯å¦å¤åˆ¶Xã€‚é»˜è®¤ä¸º Trueã€‚
- n_jobs: ç”¨äºè®¡ç®—çš„ä½œä¸šæ•°ã€‚é»˜è®¤ä¸º Noneï¼Œè¡¨ç¤ºä½¿ç”¨1ä¸ªä½œä¸šã€‚å¦‚æœè®¾ç½®ä¸º -1ï¼Œåˆ™ä½¿ç”¨æ‰€æœ‰CPUã€‚

## é€»è¾‘å›å½’

### Sigmoid å‡½æ•°

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=ODFmZjE0Y2QyYzQ3MTg5OTFlMmNkMjBlMTczZGZkMjhfZ3NwQ0NSZHZ0S3o4UlczMzVKbDJpZnU0UmYzb1J1b3BfVG9rZW46R2lEQmJ4c1Vab2Ztc3J4TURJcmM5cGhJbklkXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

å®ç°sigmodå‡½æ•°

```Python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# ç¤ºä¾‹
x = np.array([5])
print(sigmoid(x)) 
```

### æ„é€ æ•°æ®é›†

```Python
import numpy as np 
#æ„é€ æ•°æ®é›†
#ç”Ÿæˆä¸€ä¸ªæ ·æœ¬é‡ä¸º100 ç‰¹å¾é‡ä¸º10çš„æ•°æ®é›†
sample=100
feature=10
X=np.random.rand(sample,10)
#æ„é€ ç›®æ ‡å€¼ y=3+2*x1+5*x2-3*x3
y=3+2*X[:,0]+5*X[:,1]-3*X[:,2]+np.random.randn(sample) # å¢åŠ ä¸€ç‚¹å™ªéŸ³
# æˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªåˆ†ç±»çš„é—®é¢˜
mean_=np.mean(y)
# ä¸€åŠçš„æ•°æ®ä½äºå¹³å‡å€¼ ä¸€åŠçš„æ•°æ®é«˜äºå¹³å‡å€¼
y=np.where(y>=mean_,1,0)
#æ ·æœ¬å°±å˜æˆäº†åˆ†ç±»äº‹å®ï¼Œæ•°æ®æ˜¯å¦æ¯”å¹³å‡å€¼å¤§
"""
array([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
       1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
       1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1])
"""

#æ„é€ æ•°æ®é›†
```

### ä½¿ç”¨sklearnåŒ…å®ç°

```Python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np 
#æ„é€ æ•°æ®é›†
#ç”Ÿæˆä¸€ä¸ªæ ·æœ¬é‡ä¸º100 ç‰¹å¾é‡ä¸º10çš„æ•°æ®é›†
sample=100
feature=10
X=np.random.rand(sample,10)
#æ„é€ ç›®æ ‡å€¼ y=3+2*x1+5*x2-3*x3
y=3+2*X[:,0]+5*X[:,1]-3*X[:,2]+np.random.randn(sample) # å¢åŠ ä¸€ç‚¹å™ªéŸ³
# æˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªåˆ†ç±»çš„é—®é¢˜
mean_=np.mean(y)
# ä¸€åŠçš„æ•°æ®ä½äºå¹³å‡å€¼ ä¸€åŠçš„æ•°æ®é«˜äºå¹³å‡å€¼
y=np.where(y>=mean_,1,0)
#æ ·æœ¬å°±å˜æˆäº†åˆ†ç±»äº‹å®ï¼Œæ•°æ®æ˜¯å¦æ¯”å¹³å‡å€¼å¤§

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# åˆ›å»ºé€»è¾‘å›å½’æ¨¡å‹
log_reg = LogisticRegression()

# è®­ç»ƒæ¨¡å‹
log_reg.fit(X_train, y_train)

# é¢„æµ‹
predictions = log_reg.predict(X_test)

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)
```

**å‚æ•°è®²è§£**

fit() å‡½æ•°ä¸­å¸¦çš„å‚æ•°

1. **åŸºæœ¬å‚æ•°**

   1. 1.  **penalty**ï¼šè¯¥å‚æ•°ç”¨äºæŒ‡å®šæ­£åˆ™åŒ–é¡¹çš„ç±»å‹ï¼Œå¯é€‰å€¼ä¸ºâ€œl1â€ã€â€œl2â€å’Œâ€œelasticnetâ€ï¼Œé»˜è®¤ä¸ºâ€œl2â€ã€‚L1æ­£åˆ™åŒ–å€¾å‘äºç”Ÿæˆç¨€ç–æƒé‡çŸ©é˜µï¼Œæœ‰åŠ©äºç‰¹å¾é€‰æ‹©ï¼›L2æ­£åˆ™åŒ–åˆ™å€¾å‘äºé¿å…æ¨¡å‹æƒé‡è¿‡å¤§ï¼Œä»è€Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

   2. 1.  **C**ï¼šæ­¤å‚æ•°æ˜¯æ­£åˆ™åŒ–å¼ºåº¦çš„å€’æ•°ï¼Œå³æƒ©ç½šç³»æ•°Î»çš„å€’æ•°ã€‚è¾ƒå°çš„Cå€¼è¡¨ç¤ºæ­£åˆ™åŒ–å¼ºåº¦æ›´å¤§ï¼Œä¼šä½¿æ¨¡å‹æ›´ç®€å•ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚

   3. 1.  **fit_intercept**ï¼šè¯¥å¸ƒå°”å‚æ•°å†³å®šæ˜¯å¦åœ¨æ¨¡å‹ä¸­è®¡ç®—æˆªè·é¡¹ï¼Œå³æ˜¯å¦éœ€è¦åç½®ï¼Œé»˜è®¤ä¸ºTrueã€‚
2. **æ±‚è§£å™¨ç›¸å…³å‚æ•°**

   1. 1.  **solver**ï¼šè¯¥å‚æ•°ç”¨äºé€‰æ‹©ä¼˜åŒ–ç®—æ³•ï¼Œä¸åŒçš„ç®—æ³•é€‚ç”¨äºä¸åŒçš„æƒ…å†µã€‚ä¾‹å¦‚ï¼Œâ€œliblinearâ€é€‚åˆå°æ•°æ®é›†æˆ–L1æ­£åˆ™åŒ–ï¼›â€œlbfgsâ€ã€â€œnewton-cgâ€å’Œâ€œsagâ€é€‚åˆå¤§æ•°æ®é›†ä¸”ä»…æ”¯æŒL2æ­£åˆ™åŒ–ï¼›â€œsagaâ€åˆ™æ—¢é€‚ç”¨äºL1ä¹Ÿé€‚ç”¨äºL2æ­£åˆ™åŒ–ã€‚

   2. 1.  **max_iter**ï¼šè¿™æ˜¯æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œç”¨äºæŒ‡å®šä¼˜åŒ–ç®—æ³•çš„æ”¶æ•›é˜ˆå€¼ï¼Œå³è¾¾åˆ°å¤šå°‘æ¬¡è¿­ä»£ååœæ­¢è®­ç»ƒã€‚
3. **å®¹å¿åº¦åŠéšæœºçŠ¶æ€**

   1. 1.  **tol**ï¼šè¯¥å‚æ•°ç”¨äºè®¾ç½®æ±‚è§£å™¨çš„å®¹å¿åº¦ï¼Œå³å¤šå°çš„å˜åŒ–ä¼šè¢«è®¤ä¸ºæ˜¯æ”¶æ•›ï¼Œä¸å†ç»§ç»­è¿­ä»£ã€‚

   2. 1.  **random_state**ï¼šè¯¥å‚æ•°ç”¨äºè®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡è¿è¡Œçš„ç»“æœå¯å¤ç°ã€‚
4. **å¤šå…ƒåˆ†ç±»ç­–ç•¥**

   1. 1.  **multi_class**ï¼šå½“å¤„ç†å¤šåˆ†ç±»é—®é¢˜æ—¶ï¼Œè¯¥å‚æ•°å†³å®šäº†é‡‡ç”¨ä½•ç§ç­–ç•¥ï¼Œ"ovr"ï¼ˆone-vs-restï¼‰æˆ–è€…"multinomial"ï¼ˆmany-vs-manyï¼‰ï¼Œå‰è€…åœ¨æ¯ä¸ªäºŒå…ƒåˆ†ç±»é—®é¢˜ä¸Šç‹¬ç«‹è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œåè€…åˆ™åŒæ—¶è€ƒè™‘æ‰€æœ‰ç±»åˆ«ã€‚
5. **å¯¹å¶åŠæƒé‡å‚æ•°**

   1. 1.  **dual**ï¼šè¯¥å‚æ•°åªåœ¨ä½¿ç”¨â€œliblinearâ€æ±‚è§£å™¨å¹¶é€‰æ‹©L2æ­£åˆ™åŒ–æ—¶æœ‰ç”¨ï¼Œå½“æ ·æœ¬æ•°å¤§äºç‰¹å¾æ•°æ—¶å»ºè®®è®¾ç½®ä¸ºFalseã€‚

   2. 1.  **class_weight**ï¼šç”¨äºè®¾å®šå„ç±»åˆ«çš„æƒé‡ï¼Œå¯ä»¥æ˜¯å­—å…¸å½¢å¼ï¼Œä¹Ÿå¯ä»¥æ˜¯â€œbalancedâ€è®©ç±»åº“è‡ªåŠ¨è®¡ç®—æƒé‡ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡çš„æ•°æ®ã€‚
6. **å…¶ä»–å‚æ•°ï¼š**

   1.  **verboseï¼š**å¯¹äºliblinearå’Œlbfgsæ±‚è§£å™¨ï¼Œå°†verboseè®¾ç½®ä¸ºä»»ä½•æ­£æ•°ä»¥è¡¨ç¤ºè¯¦ç»†ç¨‹åº¦ã€‚ç”¨äºå¼€å¯/å…³é—­è¿­ä»£ä¸­é—´è¾“å‡ºçš„æ—¥å¿—ã€‚
7. **n_jobs**ï¼šå¦‚æœmulti_class =â€˜ovrâ€™ï¼Œåˆ™åœ¨å¯¹ç±»è¿›è¡Œå¹¶è¡ŒåŒ–æ—¶ä½¿ç”¨çš„CPUæ•°é‡ã€‚ æ— è®ºæ˜¯å¦æŒ‡å®šâ€™multi_classâ€™ï¼Œå½“`solver`è®¾ç½®ä¸ºâ€™liblinearâ€™æ—¶ï¼Œéƒ½ä¼šå¿½ç•¥æ­¤å‚æ•°ã€‚ å¦‚æœç»™å®šå€¼-1ï¼Œåˆ™ä½¿ç”¨æ‰€æœ‰CPUã€‚

## 3.4çº¿æ€§åˆ¤åˆ«åˆ†æ

```Python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®é›†
iris = load_iris()
X = iris.data
y = iris.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºLDAæ¨¡å‹
lda = LinearDiscriminantAnalysis()

# è®­ç»ƒæ¨¡å‹
lda.fit(X_train, y_train)

# é¢„æµ‹æµ‹è¯•é›†
y_pred = lda.predict(X_test)

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(y_test, y_pred)
print("LDAåˆ†ç±»å™¨çš„å‡†ç¡®ç‡ï¼š", accuracy)
```

## 3.5å¤šåˆ†ç±»å­¦ä¹ 

```Python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†
iris = load_iris()
X = iris.data
y = iris.target

# å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºé€»è¾‘å›å½’æ¨¡å‹
logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)

# è®­ç»ƒæ¨¡å‹
logreg.fit(X_train, y_train)

# é¢„æµ‹æµ‹è¯•é›†
y_pred = logreg.predict(X_test)

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}".format(accuracy))
```

## 3.6ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜

```Plain
make_classification(
                    n_samples,
                    n_features,
                    n_informative,
                    n_redundant,
                    n_repeated,
                    n_classes,
                    n_clusters_per_class,
                    weights,
                    flip_y,
                    class_sep,
                    hypercube,
                    shift,
                    scale,
                    shuffle,
                    random_state   
)  
```

1. n_samples: ä¸€ä¸ªæ•´æ•°è¡¨ç¤ºå°†è¦ç”Ÿæˆçš„æ•°æ®æ€»é‡ã€‚é»˜è®¤å€¼ä¸º100ã€‚
2. n_features: ä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºç‰¹å¾çš„æ•°é‡ã€‚åœ¨make_classificationä¸­é»˜è®¤å€¼ä¸º20ï¼Œåœ¨make_regressionä¸­é»˜è®¤å€¼ä¸º100ã€‚
3. n_informative: ä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºç‰¹å¾ä¸­æ¯”è¾ƒé‡è¦çš„ç‰¹å¾çš„æ•°é‡ï¼ˆå³å¯ä»¥æä¾›æ›´å¤šä¿¡æ¯é‡çš„ç‰¹å¾æ•°é‡ï¼‰ã€‚åœ¨make_classificationä¸­é»˜è®¤å€¼ä¸º2ï¼Œåœ¨make_regressionä¸­é»˜è®¤å€¼ä¸º10ã€‚
4. n_redundant: ä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºç‰¹å¾ä¸­å†—ä½™ç‰¹å¾çš„æ•°é‡ï¼ˆå³ä¸èƒ½æä¾›æ›´å¤šä¿¡æ¯é‡çš„ç‰¹å¾æ•°é‡ï¼‰ã€‚åœ¨make_classificationä¸­é»˜è®¤å€¼ä¸º2ã€‚
5. n_repeated: ä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºç‰¹å¾ä¸­é‡å¤ç‰¹å¾çš„æ•°é‡ã€‚å¯ä»¥æ¨¡æ‹Ÿå®é™…é—®é¢˜ä¸­å› ä¸º[æ•°æ®æå–](https://zhida.zhihu.com/search?q=æ•°æ®æå–&zhida_source=entity&is_preview=1)ä¸å¥½é€ æˆçš„æ•°æ®é‡å¤é—®é¢˜ã€‚åœ¨make_classificationä¸­é»˜è®¤å€¼ä¸º0ã€‚
6. n_classes: ä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºåˆ†ç±»é—®é¢˜ä¸­çš„ç›®æ ‡ç±»å‹çš„æ•°é‡ã€‚åœ¨make_classificationä¸­é»˜è®¤å€¼ä¸º2ã€‚
7. n_clusters_per_classï¼šä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºåˆ†ç±»é—®é¢˜ä¸­æ¯ç±»æ‹¥æœ‰çš„æ•°æ®ç°‡çš„æ•°é‡ã€‚åœ¨make_classificationä¸­é»˜è®¤å€¼ä¸º2ã€‚
8. weights: [æµ®ç‚¹æ•°](https://zhida.zhihu.com/search?q=æµ®ç‚¹æ•°&zhida_source=entity&is_preview=1)çš„åˆ—è¡¨ï¼Œè¡¨ç¤ºæ¯ä¸€ç±»æ•°æ®å æ€»æ•°æ®çš„æ¯”é‡ã€‚æ³¨æ„å½“åˆ—è¡¨ä¸­æ‰€æœ‰æµ®ç‚¹æ•°çš„å€¼ä¹‹å’Œå¤§äº1æ—¶ï¼Œå¯èƒ½ä¼šäº§ç”Ÿæ„æƒ³ä¸åˆ°çš„ç»“æœã€‚åœ¨make_classificationä¸­é»˜è®¤å€¼ä¸ºNoneã€‚
9. flip_yï¼šä¸€ä¸ªæµ®ç‚¹æ•°ï¼Œè¡¨ç¤º[å™ªéŸ³å€¼](https://zhida.zhihu.com/search?q=å™ªéŸ³å€¼&zhida_source=entity&is_preview=1)ã€‚è¿™ä¸ªæ•°è¶Šå¤§å°±ä¼šä½¿åˆ†ç±»æ›´å›°éš¾ã€‚åœ¨make_classificationä¸­é»˜è®¤å€¼ä¸º0.01ã€‚
10. class_sepï¼šä¸€ä¸ªæµ®ç‚¹æ•°ï¼Œè¡¨ç¤ºç±»ä¸ç±»ä¹‹é—´çš„é—´è·ã€‚è¿™ä¸ªå€¼è¶Šå¤§å°±ä¼šä½¿åˆ†ç±»æ›´å®¹æ˜“ã€‚åœ¨make_classificationä¸­é»˜è®¤å€¼ä¸º0.01ã€‚
11. hypercube: ä¸€ä¸ª[å¸ƒå°”å€¼](https://zhida.zhihu.com/search?q=å¸ƒå°”å€¼&zhida_source=entity&is_preview=1)ï¼Œå½“ä¸ºçœŸæ—¶ï¼Œè¡¨ç¤ºæ•°æ®ç°‡æ˜¯ä»[è¶…ç«‹æ–¹ä½“](https://zhida.zhihu.com/search?q=è¶…ç«‹æ–¹ä½“&zhida_source=entity&is_preview=1)ï¼ˆæƒ³è±¡ä¸€ä¸‹é—®é¢˜ç©ºé—´ï¼ŒäºŒç»´é—®é¢˜å°±æ˜¯æ­£æ–¹å½¢ï¼Œä¸‰ç»´å°±æ˜¯ç«‹æ–¹ä½“ï¼Œæ›´é«˜ç»´å°±æ˜¯è¶…ç«‹æ–¹ä½“äº†ï¼‰çš„é¡¶ç‚¹å¼€å§‹äº§ç”Ÿçš„ã€‚å¦åˆ™å°±è¡¨ç¤ºæ•°æ®ç°‡æ˜¯ä»éšæœºçš„å¤šå¹³é¢ä½“çš„é¡¶ç‚¹ä¸Šç”Ÿæˆçš„ã€‚è¯´å¾—æ›´ç›´ç™½ä¸€äº›çš„è¯ï¼Œå½“è¿™ä¸ªå€¼ä¸ºçœŸæ—¶ï¼Œç”Ÿæˆçš„æ•°æ®ä¼šæ›´å‡åŒ€ä¸€äº›ã€‚åœ¨make_classificationä¸­é»˜è®¤å€¼ä¸ºTrueã€‚
12. shiftï¼šä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸€ä¸ªé•¿åº¦ä¸ºn_featuresçš„æµ®ç‚¹æ•°ç»„æˆ–è€…Noneã€‚è¡¨ç¤ºå°†ç‰¹å¾å€¼é€šè¿‡æŸä¸ªå€¼è¿›è¡Œå¹³ç§»ï¼Œä¸ç„¶ç”Ÿæˆçš„ç‰¹å¾å€¼å°±åˆ†å¸ƒåœ¨0ç‚¹çš„å‘¨å›´äº†ã€‚åœ¨make_classificationä¸­é»˜è®¤å€¼ä¸º0.0ã€‚
13. scaleï¼šä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸€ä¸ªé•¿åº¦ä¸ºn_featuresçš„æµ®ç‚¹æ•°ç»„æˆ–è€…Noneã€‚è¡¨ç¤ºå°†ç‰¹å¾å€¼ä¸æŸä¸ªå€¼ç›¸ä¹˜åçš„ç»“æœèµ‹å€¼ç»™è¿™ä¸ªç‰¹å¾å€¼ï¼Œæ³¨æ„æ˜¯å…ˆå‘ç”Ÿshiftå†scaleï¼Œå­¦è¿‡[çº¿æ€§ä»£æ•°](https://zhida.zhihu.com/search?q=çº¿æ€§ä»£æ•°&zhida_source=entity&is_preview=1)çš„åŒå­¦è‚¯å®šä¸€ä¸‹å­å°±å¯ä»¥å‘ç°è¿™å°±æ˜¯å¯¹ç‰¹å¾å€¼åšä¸€ä¸ªä¸€ç»´[çº¿æ€§å˜æ¢](https://zhida.zhihu.com/search?q=çº¿æ€§å˜æ¢&zhida_source=entity&is_preview=1)ã€‚åœ¨make_classificationä¸­é»˜è®¤å€¼ä¸º1.0ã€‚
14. shuffle: ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦è¦æ‰“ä¹±ç”Ÿæˆçš„æ•°æ®ã€‚åœ¨make_regressionä¸­é»˜è®¤ä¸ºTrueã€‚
15. random_stateï¼šNoneæˆ–è€…ä¸€ä¸ªæ•´æ•°ï¼Œå½“è¾“å…¥ä¸ºä¸€ä¸ªæ•´æ•°æ—¶ï¼Œè¡¨ç¤ºè¿™æ¬¡ç”Ÿæˆæ•°æ®è¿‡ç¨‹çš„éšæœºå› å­ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœä¸¤æ¬¡ç”Ÿæˆæ•°æ®æ—¶ï¼Œå¦‚æœrandom_stateæ˜¯åŒä¸€ä¸ªæ•´æ•°ï¼Œä¸”å…¶ä»–å‚æ•°éƒ½ç›¸åŒï¼Œåˆ™ç”Ÿæˆçš„æ•°æ®æ˜¯ä¸€æ ·çš„ã€‚

```Python
from sklearn.datasets import make_classification

X, y = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, n_classes=3, random_state=42,n_clusters_per_class=1)
print("ç‰¹å¾æ•°æ®ï¼š\n", X)
print("æ ‡ç­¾æ•°æ®ï¼š\n", y)
```

### æ¬ é‡‡æ ·

```Python
from sklearn.datasets import make_classification
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter

# åˆ›å»ºä¸€ä¸ªä¸å¹³è¡¡çš„æ•°æ®é›†
X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)

# æ‰“å°åŸå§‹æ•°æ®é›†çš„ç±»åˆ«åˆ†å¸ƒ
print("åŸå§‹æ•°æ®é›†ç±»åˆ«åˆ†å¸ƒï¼š", Counter(y))

# å®ä¾‹åŒ–RandomUnderSamplerå¯¹è±¡
rus = RandomUnderSampler(random_state=42)

# å¯¹æ•°æ®é›†è¿›è¡Œæ¬ é‡‡æ ·
X_resampled, y_resampled = rus.fit_resample(X, y)

# æ‰“å°æ¬ é‡‡æ ·åçš„æ•°æ®é›†ç±»åˆ«åˆ†å¸ƒ
print("æ¬ é‡‡æ ·åæ•°æ®é›†ç±»åˆ«åˆ†å¸ƒï¼š", Counter(y_resampled))
```

### è¿‡é‡‡æ ·

```Python
from sklearn.datasets import make_classification
from imblearn.over_sampling import RandomOverSampler
from collections import Counter

# åˆ›å»ºä¸€ä¸ªä¸å¹³è¡¡çš„æ•°æ®é›†
X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)

# æ‰“å°åŸå§‹æ•°æ®é›†çš„ç±»åˆ«åˆ†å¸ƒ
print("åŸå§‹æ•°æ®é›†ç±»åˆ«åˆ†å¸ƒï¼š", Counter(y))

# å®ä¾‹åŒ–RandomOverSamplerå¯¹è±¡
ros = RandomOverSampler(random_state=42)

# å¯¹æ•°æ®é›†è¿›è¡Œè¿‡é‡‡æ ·
X_resampled, y_resampled = ros.fit_resample(X, y)

# æ‰“å°è¿‡é‡‡æ ·åçš„æ•°æ®é›†ç±»åˆ«åˆ†å¸ƒ
print("è¿‡é‡‡æ ·åæ•°æ®é›†ç±»åˆ«åˆ†å¸ƒï¼š", Counter(y_resampled))
```

### é˜ˆå€¼ç§»åŠ¨

å¯ä»¥é€šè¿‡è®¾ç½®`class_weight`å‚æ•°æ¥è°ƒæ•´ç±»åˆ«æƒé‡ï¼Œä»¥å¹³è¡¡ä¸åŒç±»åˆ«çš„æŸå¤±è´¡çŒ®

**class_weight**ï¼šç”¨äºè®¾å®šå„ç±»åˆ«çš„æƒé‡ï¼Œå¯ä»¥æ˜¯å­—å…¸å½¢å¼ï¼Œä¹Ÿå¯ä»¥æ˜¯â€œbalancedâ€è®©ç±»åº“è‡ªåŠ¨è®¡ç®—æƒé‡ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡çš„æ•°æ®ã€‚æˆ–è€…å¯ä»¥ç”¨å­—å…¸çš„å½¢å¼ä¼ å‚ï¼Œä¾‹å¦‚class_weight = {0:1,1:3}

```Python
#åˆ†ç±»çš„æ—¶å€™ï¼Œå½“ä¸åŒç±»åˆ«çš„æ ·æœ¬é‡å·®å¼‚å¾ˆå¤§æ—¶ï¼Œå¾ˆå®¹æ˜“å½±å“åˆ†ç±»ç»“æœï¼Œå› æ­¤è¦ä¹ˆæ¯ä¸ªç±»åˆ«çš„æ•°æ®é‡å¤§è‡´ç›¸åŒï¼Œè¦ä¹ˆå°±è¦è¿›è¡Œæ ¡æ­£ã€‚
#sklearnçš„åšæ³•å¯ä»¥æ˜¯åŠ æƒï¼ŒåŠ æƒå°±è¦æ¶‰åŠåˆ°class_weightå’Œsample_weight
#å½“ä¸è®¾ç½®class_weightå‚æ•°æ—¶ï¼Œé»˜è®¤å€¼æ˜¯æ‰€æœ‰ç±»åˆ«çš„æƒå€¼ä¸º1

#é‚£ä¹ˆ'balanced'çš„è®¡ç®—æ–¹æ³•æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ
import numpy as np

y = [0,0,0,0,0,0,0,0,1,1,1,1,1,1,2,2]  #æ ‡ç­¾å€¼ï¼Œä¸€å…±16ä¸ªæ ·æœ¬

a = np.bincount(y)  # array([8, 6, 2], dtype=int64) è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡
aa = 1/a  #å€’æ•° array([0.125     , 0.16666667, 0.5       ])
print(aa)

from sklearn.utils.class_weight import compute_class_weight 
y = [0,0,0,0,0,0,0,0,1,1,1,1,1,1,2,2]

# è®¡ç®—ç±»åˆ«æƒé‡
class_weights = compute_class_weight('balanced', classes=[0, 1,2], y=y)
print("ç±»åˆ«æƒé‡ï¼š", class_weights) # [0.66666667 0.88888889 2.66666667]


#weight_ = n_samples / (n_classes * np.bincount(y))

print(16/(3*8))  #è¾“å‡º 0.6666666666666666
print(16/(3*6))  #è¾“å‡º 0.8888888888888888
print(16/(3*2))  #è¾“å‡º 2.6666666666666665
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# åˆ›å»ºä¸€ä¸ªä¸å¹³è¡¡çš„æ•°æ®é›†
X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºé€»è¾‘å›å½’æ¨¡å‹ï¼Œå¹¶è®¾ç½®class_weightä¸º'balanced'
#class_weight å¯ä»¥è®¾ç½®ä¸º {0:99:ï¼Œ1:1}
logreg = LogisticRegression(class_weight='balanced', solver='liblinear', max_iter=1000)

# è®­ç»ƒæ¨¡å‹
logreg.fit(X_train, y_train)

# é¢„æµ‹æµ‹è¯•é›†
y_pred = logreg.predict(X_test)

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(y_test, y_pred)
print("é€»è¾‘å›å½’æ¨¡å‹çš„å‡†ç¡®ç‡ï¼š", accuracy)
```

# ç¬¬4ç« å†³ç­–æ ‘

sklearn.datasets ä¸­ä¸»è¦åŒ…å«äº†ä¸€äº›å…¬å…±æ•°æ®é›†

- load_iris(): åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„å¤šç±»åˆ†ç±»æ•°æ®é›†ã€‚
- load_digits(): åŠ è½½æ‰‹å†™æ•°å­—æ•°æ®é›†ï¼Œæ¯ä¸ªå®ä¾‹éƒ½æ˜¯ä¸€å¼ 8x8çš„æ•°å­—å›¾åƒåŠå…¶å¯¹åº”çš„æ•°å­—ç±»åˆ«ã€‚
- load_boston(): åŠ è½½æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå›å½’é—®é¢˜çš„æ•°æ®é›†ã€‚
- load_breast_cancer(): åŠ è½½ä¹³è…ºç™Œæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜çš„æ•°æ®é›†ã€‚
- load_diabetes(): åŠ è½½ç³–å°¿ç—…æ•°æ®é›†ï¼Œè¿™ä¸ªæ•°æ®é›†å¯ä»¥ç”¨äºå›å½’åˆ†æã€‚

sklearnä¸­å†³ç­–æ ‘çš„ç”¨æ³•

- from sklearn.tree import DecisionTreeClassifier  åˆ†ç±»æ ‘
- from sklearn.tree import DecisionTreeRegressor å›å½’æ ‘
- from sklearn.tree import plot_tree ç”»å†³ç­–æ ‘

```Python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import plot_tree
from matplotlib import rcParams
# åŠ è½½æ•°æ®é›†
iris = load_iris()
X = iris.data
y = iris.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºå†³ç­–æ ‘æ¨¡å‹
clf = DecisionTreeClassifier()

# è®­ç»ƒæ¨¡å‹
clf.fit(X_train, y_train)

# é¢„æµ‹
predictions = clf.predict(X_test)

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)


####ç”»å†³ç­–æ ‘æ›²çº¿
rcParams['figure.figsize'] = 16, 10  # è®¾ç½®ä¸º16å®½, 10é«˜çš„å›¾åƒå¤§å°
plot_tree(clf)
```

å†³ç­–æ ‘æ¨¡å‹çš„å‚æ•°ï¼š

1. **åŸºæœ¬å‚æ•°**
   1. **Criterion**ï¼šè¯¥å‚æ•°ç”¨äºæŒ‡å®šåœ¨åˆ†è£‚èŠ‚ç‚¹æ—¶ä½¿ç”¨çš„æ ‡å‡†ã€‚å¯é€‰çš„å€¼æœ‰"gini"å’Œ"entropy"ï¼Œå…¶ä¸­"gini"è¡¨ç¤ºä½¿ç”¨åŸºå°¼ä¸çº¯åº¦ï¼Œè€Œ"entropy"åˆ™è¡¨ç¤ºä½¿ç”¨ä¿¡æ¯å¢ç›Šã€‚é»˜è®¤å€¼ä¸º"gini"ã€‚
   2. **Splitter**ï¼šè¯¥å‚æ•°ç”¨äºè®¾ç½®åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šé€‰æ‹©åˆ†è£‚çš„ç­–ç•¥ã€‚å¯é€‰çš„å€¼æœ‰"best"å’Œ"random"ï¼Œå…¶ä¸­"best"è¡¨ç¤ºä»æ‰€æœ‰ç‰¹å¾ä¸­é€‰æ‹©æœ€ä¼˜çš„è¿›è¡Œåˆ†è£‚ï¼Œè€Œ"random"åˆ™è¡¨ç¤ºéšæœºé€‰æ‹©ä¸€ä¸ªç‰¹å¾å­é›†è¿›è¡Œåˆ†è£‚ã€‚é»˜è®¤å€¼ä¸º"best"ã€‚
   3. **Max depth**ï¼šè¯¥å‚æ•°ç”¨äºè®¾ç½®å†³ç­–æ ‘çš„æœ€å¤§æ·±åº¦ã€‚å¦‚æœè®¾ç½®ä¸ºNoneï¼Œé‚£ä¹ˆå†³ç­–æ ‘ä¼šæ‰©å±•åˆ°æ‰€æœ‰å¶å­éƒ½å˜å¾—çº¯å‡€æˆ–ç›´åˆ°æ‰€æœ‰å¶å­èŠ‚ç‚¹åŒ…å«çš„æ ·æœ¬æ•°å°äºmin_samples_splitä¸ºæ­¢ã€‚è¿™å¯ä»¥é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆã€‚
   4. **Min samples split**ï¼šè¯¥å‚æ•°ç”¨äºè®¾ç½®ä¸€ä¸ªèŠ‚ç‚¹åœ¨åˆ†è£‚æ—¶æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°é‡ã€‚å¦‚æœè¿™ä¸ªå€¼è¾ƒå¤§ï¼Œå¯ä»¥å‡å°‘æ¨¡å‹çš„è¿‡æ‹Ÿåˆé£é™©ã€‚
   5. **Min samples leaf**ï¼šè¯¥å‚æ•°ç”¨äºè®¾ç½®ä¸€ä¸ªå¶å­èŠ‚ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°é‡ã€‚è®¾ç½®ä¸€ä¸ªåˆé€‚çš„å€¼å¯ä»¥é¿å…å¶å­èŠ‚ç‚¹è¿‡äºç¨€ç–ï¼Œä»è€Œé™ä½è¿‡æ‹Ÿåˆçš„é£é™©ã€‚
   6. **Min weight fraction leaf**ï¼šè¯¥å‚æ•°ç”¨äºè®¾ç½®å¶èŠ‚ç‚¹ä¸­æ ·æœ¬æƒé‡æ€»å’Œçš„æœ€å°æ¯”ä¾‹ã€‚å¦‚æœæœªæä¾›sample_weightï¼Œåˆ™æ‰€æœ‰æƒé‡ç›¸ç­‰ã€‚
   7. **Max features**ï¼šè¯¥å‚æ•°ç”¨äºè®¾ç½®å¯»æ‰¾æœ€ä½³åˆ†è£‚æ—¶è¦è€ƒè™‘çš„ç‰¹å¾æ•°é‡ã€‚å¯ä»¥æ˜¯intã€floatæˆ–"auto"ã€"sqrt"ã€"log2"ä¹‹ä¸€ã€‚å¦‚æœæ˜¯intï¼Œè€ƒè™‘max_featuresä¸ªç‰¹å¾ï¼›å¦‚æœæ˜¯floatï¼Œè€ƒè™‘ max_features ä¹˜ä»¥ç‰¹å¾æ€»æ•°ï¼›å¦‚æœæ˜¯"auto"ï¼Œåˆ™è€ƒè™‘ç‰¹å¾æ€»æ•°çš„å¹³æ–¹æ ¹ã€‚
2. **é«˜çº§å‚æ•°**
   1. **Random state**ï¼šè¯¥å‚æ•°æ˜¯éšæœºç§å­ï¼Œç”¨äºæ§åˆ¶åˆ†è£‚ç‰¹å¾çš„éšæœºæ€§ã€‚è¿™å¯ä»¥ç¡®ä¿åœ¨åŒä¸€ä»½æ•°æ®ä¸Šå¤šæ¬¡è¿è¡Œç®—æ³•æ—¶èƒ½å¾—åˆ°ç›¸åŒçš„ç»“æœã€‚
   2. **Max leaf nodes**ï¼šè¯¥å‚æ•°ç”¨äºè®¾ç½®æœ€å¤§å¶å­èŠ‚ç‚¹çš„æ•°é‡ã€‚é€šè¿‡é™åˆ¶å¶å­èŠ‚ç‚¹æ•°é‡ï¼Œå¯ä»¥é˜²æ­¢å†³ç­–æ ‘è¿‡åº¦ç”Ÿé•¿ã€‚
   3. **Min impurity decrease**ï¼šè¯¥å‚æ•°ç”¨äºè®¾ç½®å¦‚æœåˆ†è£‚åæ‚è´¨å‡å°‘é‡å¤§äºè¯¥å€¼ï¼Œåˆ™è¿›è¡Œåˆ†è£‚ã€‚è¿™å¯ä»¥ç”¨æ¥æ§åˆ¶å†³ç­–æ ‘çš„ç”Ÿé•¿é€Ÿåº¦ã€‚
   4. **Class weight**ï¼šè¯¥å‚æ•°ç”¨äºè®¾å®šå„ç±»åˆ«çš„æƒé‡ï¼Œå¯ä»¥æ˜¯å­—å…¸å½¢å¼{class_label: weight}ï¼Œä¹Ÿå¯ä»¥æ˜¯"balanced"ï¼Œåè€…ä¼šæ ¹æ®æ ·æœ¬æ•°é‡å’Œç±»åˆ«è‡ªåŠ¨è°ƒæ•´æƒé‡

### å‚æ•°ä¼˜åŒ–

GridSearchCV

1. estimator é€‰æ‹©ä½¿ç”¨çš„åˆ†ç±»å™¨ï¼Œä¹Ÿå°±æ˜¯éœ€è¦ä¼˜åŒ–å‚æ•°çš„æ¨¡å‹
2. param_grid éœ€è¦æœ€ä¼˜åŒ–çš„å‚æ•°çš„å–å€¼ï¼Œå€¼ä¸ºå­—å…¸æˆ–è€…åˆ—è¡¨
3. scoring=None 
4. æ¨¡å‹è¯„ä»·æ ‡å‡†ï¼Œé»˜è®¤None,è¿™æ—¶éœ€è¦ä½¿ç”¨scoreå‡½æ•°ï¼›æˆ–è€…å¦‚scoring='roc_auc'ï¼Œ
5. n_jobs=1 n_jobs: å¹¶è¡Œæ•°ï¼Œintï¼šä¸ªæ•°,-1ï¼šè·ŸCPUæ ¸æ•°ä¸€è‡´, 1:é»˜è®¤å€¼
6. cv äº¤å‰éªŒè¯å‚æ•°ï¼Œé»˜è®¤Noneï¼Œä½¿ç”¨ä¸‰æŠ˜äº¤å‰éªŒè¯ã€‚æŒ‡å®šfoldæ•°é‡ï¼Œé»˜è®¤ä¸º3ï¼Œä¹Ÿå¯ä»¥æ˜¯yieldäº§ç”Ÿè®­ç»ƒ/æµ‹è¯•æ•°æ®çš„ç”Ÿæˆå™¨ã€‚
7. verbose æ—¥å¿—å†—é•¿åº¦ï¼Œintï¼šå†—é•¿åº¦ï¼Œ0ï¼šä¸è¾“å‡ºè®­ç»ƒè¿‡ç¨‹ï¼Œ1ï¼šå¶å°”è¾“å‡ºï¼Œ>1ï¼šå¯¹æ¯ä¸ªå­æ¨¡å‹éƒ½è¾“å‡ºã€‚

 

è¿›è¡Œé¢„æµ‹çš„å¸¸ç”¨æ–¹æ³•å’Œå±æ€§

grid.fit()ï¼šè¿è¡Œç½‘æ ¼æœç´¢

grid_scores_ï¼šç»™å‡ºä¸åŒå‚æ•°æƒ…å†µä¸‹çš„è¯„ä»·ç»“æœ

best_params_ï¼šæè¿°äº†å·²å–å¾—æœ€ä½³ç»“æœçš„å‚æ•°çš„ç»„åˆ

best_score_ï¼šæˆå‘˜æä¾›ä¼˜åŒ–è¿‡ç¨‹æœŸé—´è§‚å¯Ÿåˆ°çš„æœ€å¥½çš„è¯„åˆ†

```Python
# å¯¼å…¥æ‰€éœ€åº“å’Œæ¨¡å—
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®é›†
iris = load_iris()
X = iris.data
y = iris.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# åˆ›å»ºå†³ç­–æ ‘æ¨¡å‹
dt = DecisionTreeClassifier()

# è®¾ç½®å‚æ•°ç½‘æ ¼
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# ä½¿ç”¨ç½‘æ ¼æœç´¢è¿›è¡Œå‚æ•°ä¼˜åŒ–
grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# è¾“å‡ºæœ€ä½³å‚æ•°
print("Best parameters found: ", grid_search.best_params_)

# ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹
best_dt = grid_search.best_estimator_
best_dt.fit(X_train, y_train)

# é¢„æµ‹æµ‹è¯•é›†å¹¶è®¡ç®—å‡†ç¡®ç‡
y_pred = best_dt.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy on test set: {:.2f}".format(accuracy))
```

# ç¬¬5ç« ç¥ç»ç½‘ç»œ

from sklearn.neural_network import MLPClassifier  #åˆ†ç±»é—®é¢˜

from sklearn.neural_network import MLPRegressor #å›å½’é—®é¢˜

```Python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®é›†
iris = load_iris()
X = iris.data
y = iris.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºå¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹
mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)

# è®­ç»ƒæ¨¡å‹
mlp.fit(X_train, y_train)

# é¢„æµ‹
predictions = mlp.predict(X_test)

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)

ï¼ˆ100ï¼‰
ï¼ˆ100,100ï¼‰
```

æ ¸å¿ƒå‚æ•°è¯´æ˜ï¼š

- **hidden_layer_sizes**ï¼šä¸€ä¸ªå…ƒç»„ï¼Œè¡¨ç¤ºéšè—å±‚ä¸­ç¥ç»å…ƒçš„æ•°é‡ã€‚ä¾‹å¦‚ï¼Œ`(100,)` è¡¨ç¤ºä¸€ä¸ªéšè—å±‚ï¼ŒåŒ…å« 100 ä¸ªç¥ç»å…ƒ
- **activation**ï¼šéšè—å±‚çš„æ¿€æ´»å‡½æ•°ã€‚é€‰é¡¹æœ‰ `'identity'`ã€`'logistic'`ã€`'tanh'`ã€`'relu'`ã€‚
- **solver**ï¼šç”¨äºæƒé‡ä¼˜åŒ–çš„ä¼˜åŒ–å™¨ã€‚`'lbfgs'` æ˜¯ä¸€ç§å‡†ç‰›é¡¿æ³•ï¼Œ`'sgd'` æ˜¯éšæœºæ¢¯åº¦ä¸‹é™ï¼Œ`'adam'` æ˜¯ä¸€ç§åŸºäºéšæœºæ¢¯åº¦çš„ä¼˜åŒ–å™¨ã€‚
- **alpha**ï¼šL2 æƒ©ç½šï¼ˆæ­£åˆ™åŒ–é¡¹ï¼‰å‚æ•°ã€‚
- **batch_size**ï¼šéšæœºä¼˜åŒ–å™¨çš„å°æ‰¹é‡å¤§å°ã€‚
- **learning_rate**ï¼šæƒé‡æ›´æ–°çš„å­¦ä¹ ç‡è°ƒåº¦ã€‚é€‰é¡¹æœ‰ `'constant'`ã€`'invscaling'`ã€`'adaptive'`ã€‚
- **max_iter**ï¼šæœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚æ±‚è§£å™¨è¿­ä»£ç›´åˆ°æ”¶æ•›æˆ–è¾¾åˆ°æ­¤è¿­ä»£æ¬¡æ•°ã€‚
- **random_state**ï¼šéšæœºæ•°ç”Ÿæˆçš„ç§å­ã€‚
- **shuffle** : æ¯æ¬¡è¿­ä»£, æ˜¯å¦æ´—ç‰Œ, å¯é€‰, ç¼ºçœTrue,ä»…é€‚ç”¨äºsgdæˆ–adam

```Python
import numpy as np
import pandas as pd
from sklearn.neural_network import MLPRegressor
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# åŠ è½½åŠ å·æˆ¿ä»·æ•°æ®é›†
data = load_diabetes()
feature = data.data
target = data.target

xtrain, xtest, ytrain, ytest = train_test_split(feature, target, train_size=0.7, random_state=421)

nn = MLPRegressor(hidden_layer_sizes=(100,100), activation="identity", solver="lbfgs", alpha=0.01)
model = nn.fit(xtrain, ytrain)
pre = model.predict(xtest)


index = 0
for w in model.coefs_:
    index += 1
    print('ç¬¬{}å±‚ç½‘ç»œå±‚:'.format(index))
    print('æƒé‡çŸ©é˜µ:', w.shape)
    print('ç³»æ•°çŸ©é˜µï¼š', w)

plt.plot(range(len(pre)), pre, color='red', label='Predicted')
plt.plot(range(len(ytest)), ytest, color='blue', label='Actual')

plt.legend()
plt.show()
```

# ç¬¬6ç« æ”¯æŒå‘é‡æœº

> æ”¯æŒå‘é‡æœºçš„ä¼˜ç‚¹æœ‰ï¼š
>
> - åœ¨é«˜ç»´ç©ºé—´é‡Œä¹Ÿéå¸¸æœ‰æ•ˆ
> - å¯¹äºæ•°æ®ç»´åº¦è¿œé«˜äºæ•°æ®æ ·æœ¬é‡çš„æƒ…å†µä¹Ÿæœ‰æ•ˆ
> - åœ¨å†³ç­–å‡½æ•°ä¸­ä½¿ç”¨è®­ç»ƒé›†çš„å­é›†(ä¹Ÿç§°ä¸ºæ”¯æŒå‘é‡)ï¼Œå› æ­¤ä¹Ÿæ˜¯å†…å­˜é«˜æ•ˆåˆ©ç”¨çš„ã€‚
> - é€šç”¨æ€§ï¼šå¯ä»¥ä¸ºå†³ç­–å‡½æ•°æŒ‡å®šä¸åŒçš„æ ¸å‡½æ•°ã€‚å·²ç»æä¾›äº†é€šç”¨æ ¸å‡½æ•°ï¼Œä½†ä¹Ÿå¯ä»¥æŒ‡å®šè‡ªå®šä¹‰æ ¸å‡½æ•°ã€‚
>
> æ”¯æŒå‘é‡æœºçš„ç¼ºç‚¹åŒ…æ‹¬ï¼š
>
> - å¦‚æœç‰¹å¾æ•°é‡è¿œè¿œå¤§äºæ ·æœ¬æ•°ï¼Œåˆ™åœ¨é€‰æ‹©æ ¸å‡½æ•°å’Œæ­£åˆ™åŒ–é¡¹æ—¶è¦é¿å…è¿‡åº¦æ‹Ÿåˆã€‚
> - SVMsä¸ç›´æ¥æä¾›æ¦‚ç‡ä¼°è®¡ï¼Œ è¿™äº›è®¡ç®—ä½¿ç”¨æ˜‚è´µçš„äº”å€äº¤å‰éªŒè¯

from sklearn.svm import SVC   åˆ†ç±»ä»»åŠ¡

from sklearn.svm import SVR   å›å½’ä»»åŠ¡

å½“ç„¶è¿˜æœ‰å…¶ä»–çš„å½¢å¼ä¾‹å¦‚ NuSVCã€LinearSVCç±»ç­‰

```Python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_breast_cancer #ä¹³è…ºç™Œ

# åŠ è½½æ•°æ®é›†
iris = load_iris()
X = iris.data
y = iris.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºæ”¯æŒå‘é‡æœºæ¨¡å‹
svm = SVC(kernel='linear', C=1, random_state=42)

# è®­ç»ƒæ¨¡å‹
svm.fit(X_train, y_train)

# é¢„æµ‹
predictions = svm.predict(X_test)

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)
```

å¸¸è§å‚æ•°ï¼š

**C** ï¼ˆfloatå‚æ•° é»˜è®¤å€¼ä¸º1.0ï¼‰ è¡¨ç¤ºé”™è¯¯é¡¹çš„æƒ©ç½šç³»æ•°Cè¶Šå¤§ï¼Œå³å¯¹åˆ†é”™æ ·æœ¬çš„æƒ©ç½šç¨‹åº¦è¶Šå¤§ï¼Œå› æ­¤åœ¨è®­ç»ƒæ ·æœ¬ä¸­å‡†ç¡®ç‡è¶Šé«˜ï¼Œä½†æ˜¯æ³›åŒ–èƒ½åŠ›é™ä½ï¼›ç›¸åï¼Œå‡å°Cçš„è¯ï¼Œå®¹è®¸è®­ç»ƒæ ·æœ¬ä¸­æœ‰ä¸€äº›è¯¯åˆ†ç±»é”™è¯¯æ ·æœ¬ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºã€‚å¯¹äºè®­ç»ƒæ ·æœ¬å¸¦æœ‰å™ªå£°çš„æƒ…å†µï¼Œä¸€èˆ¬é‡‡ç”¨åè€…ï¼ŒæŠŠè®­ç»ƒæ ·æœ¬é›†ä¸­é”™è¯¯åˆ†ç±»çš„æ ·æœ¬ä½œä¸ºå™ªå£°ã€‚

**kernel** ï¼ˆstrå‚æ•° é»˜è®¤ä¸ºâ€˜rbfâ€™ï¼‰ è¯¥å‚æ•°ç”¨äºé€‰æ‹©æ¨¡å‹æ‰€ä½¿ç”¨çš„æ ¸å‡½æ•°ï¼Œç®—æ³•ä¸­å¸¸ç”¨çš„æ ¸å‡½æ•°æœ‰ï¼š -- linearï¼šçº¿æ€§æ ¸å‡½æ•° --  polyï¼šå¤šé¡¹å¼æ ¸å‡½æ•° --rbfï¼šå¾„åƒæ ¸å‡½æ•°/é«˜æ–¯æ ¸ --sigmodï¼šsigmodæ ¸å‡½æ•° --precomputedï¼šæ ¸çŸ©é˜µï¼Œè¯¥çŸ©é˜µè¡¨ç¤ºè‡ªå·±äº‹å…ˆè®¡ç®—å¥½çš„ï¼Œè¾“å…¥åç®—æ³•å†…éƒ¨å°†ä½¿ç”¨ä½ æä¾›çš„çŸ©é˜µè¿›è¡Œè®¡ç®—

**degree** ï¼ˆintå‹å‚æ•° é»˜è®¤ä¸º3ï¼‰ è¯¥å‚æ•°åªå¯¹'kernel=poly'(å¤šé¡¹å¼æ ¸å‡½æ•°)æœ‰ç”¨ï¼Œæ˜¯æŒ‡å¤šé¡¹å¼æ ¸å‡½æ•°çš„é˜¶æ•°nï¼Œå¦‚æœç»™çš„æ ¸å‡½æ•°å‚æ•°æ˜¯å…¶ä»–æ ¸å‡½æ•°ï¼Œåˆ™ä¼šè‡ªåŠ¨å¿½ç•¥è¯¥å‚æ•°ã€‚

**gamma** ï¼ˆfloatå‚æ•° é»˜è®¤ä¸ºautoï¼‰ è¯¥å‚æ•°ä¸ºæ ¸å‡½æ•°ç³»æ•°ï¼Œåªå¯¹â€˜rbfâ€™,â€˜polyâ€™,â€˜sigmodâ€™æœ‰æ•ˆã€‚å¦‚æœgammaè®¾ç½®ä¸ºautoï¼Œä»£è¡¨å…¶å€¼ä¸ºæ ·æœ¬ç‰¹å¾æ•°çš„å€’æ•°ï¼Œå³1/n_featuresï¼Œä¹Ÿæœ‰å…¶ä»–å€¼å¯è®¾å®šã€‚

**coef0**:ï¼ˆfloatå‚æ•° é»˜è®¤ä¸º0.0ï¼‰ è¯¥å‚æ•°è¡¨ç¤ºæ ¸å‡½æ•°ä¸­çš„ç‹¬ç«‹é¡¹ï¼Œåªæœ‰å¯¹â€˜polyâ€™å’Œâ€˜sigmodâ€™æ ¸å‡½æ•°æœ‰ç”¨ï¼Œæ˜¯æŒ‡å…¶ä¸­çš„å‚æ•°cã€‚

**probability**ï¼ˆ boolå‚æ•° é»˜è®¤ä¸ºFalseï¼‰ è¯¥å‚æ•°è¡¨ç¤ºæ˜¯å¦å¯ç”¨æ¦‚ç‡ä¼°è®¡ã€‚ è¿™å¿…é¡»åœ¨è°ƒç”¨fit()ä¹‹å‰å¯ç”¨ï¼Œå¹¶ä¸”ä¼šä½¿fit()æ–¹æ³•é€Ÿåº¦å˜æ…¢ã€‚

**shrinkintol: floatå‚æ•° é»˜è®¤ä¸º1e^-3g**ï¼ˆboolå‚æ•° é»˜è®¤ä¸ºTrueï¼‰ è¯¥å‚æ•°è¡¨ç¤ºæ˜¯å¦é€‰ç”¨å¯å‘å¼æ”¶ç¼©æ–¹å¼ã€‚

**tol**ï¼ˆ floatå‚æ•° é»˜è®¤ä¸º1e^-3ï¼‰ svmåœæ­¢è®­ç»ƒçš„è¯¯å·®ç²¾åº¦ï¼Œä¹Ÿå³é˜ˆå€¼ã€‚

**cache_size**ï¼ˆfloatå‚æ•° é»˜è®¤ä¸º200ï¼‰ è¯¥å‚æ•°è¡¨ç¤ºæŒ‡å®šè®­ç»ƒæ‰€éœ€è¦çš„å†…å­˜ï¼Œä»¥MBä¸ºå•ä½ï¼Œé»˜è®¤ä¸º200MBã€‚

**class_weight**ï¼ˆå­—å…¸ç±»å‹æˆ–è€…â€˜balanceâ€™å­—ç¬¦ä¸²ã€‚é»˜è®¤ä¸ºNoneï¼‰ è¯¥å‚æ•°è¡¨ç¤ºç»™æ¯ä¸ªç±»åˆ«åˆ†åˆ«è®¾ç½®ä¸åŒçš„æƒ©ç½šå‚æ•°Cï¼Œå¦‚æœæ²¡æœ‰ç»™ï¼Œåˆ™ä¼šç»™æ‰€æœ‰ç±»åˆ«éƒ½ç»™C=1ï¼Œå³å‰é¢å‚æ•°æŒ‡å‡ºçš„å‚æ•°Cã€‚å¦‚æœç»™å®šå‚æ•°â€˜balanceâ€™ï¼Œåˆ™ä½¿ç”¨yçš„å€¼è‡ªåŠ¨è°ƒæ•´ä¸è¾“å…¥æ•°æ®ä¸­çš„ç±»é¢‘ç‡æˆåæ¯”çš„æƒé‡ã€‚

**verbose** ï¼ˆ boolå‚æ•° é»˜è®¤ä¸ºFalseï¼‰ è¯¥å‚æ•°è¡¨ç¤ºæ˜¯å¦å¯ç”¨è¯¦ç»†è¾“å‡ºã€‚æ­¤è®¾ç½®åˆ©ç”¨libsvmä¸­çš„æ¯ä¸ªè¿›ç¨‹è¿è¡Œæ—¶è®¾ç½®ï¼Œå¦‚æœå¯ç”¨ï¼Œå¯èƒ½æ— æ³•åœ¨å¤šçº¿ç¨‹ä¸Šä¸‹æ–‡ä¸­æ­£å¸¸å·¥ä½œã€‚ä¸€èˆ¬æƒ…å†µéƒ½è®¾ä¸ºFalseï¼Œä¸ç”¨ç®¡å®ƒã€‚

**max_iter** ï¼ˆintå‚æ•° é»˜è®¤ä¸º-1ï¼‰ è¯¥å‚æ•°è¡¨ç¤ºæœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œå¦‚æœè®¾ç½®ä¸º-1åˆ™è¡¨ç¤ºä¸å—é™åˆ¶ã€‚

**random_state**ï¼ˆintï¼ŒRandomState instance ï¼ŒNone é»˜è®¤ä¸ºNoneï¼‰ è¯¥å‚æ•°è¡¨ç¤ºåœ¨æ··æ´—æ•°æ®æ—¶æ‰€ä½¿ç”¨çš„ä¼ªéšæœºæ•°å‘ç”Ÿå™¨çš„ç§å­ï¼Œå¦‚æœé€‰intï¼Œåˆ™ä¸ºéšæœºæ•°ç”Ÿæˆå™¨ç§å­ï¼›å¦‚æœé€‰RandomState instanceï¼Œåˆ™ä¸ºéšæœºæ•°ç”Ÿæˆå™¨ï¼›å¦‚æœé€‰None,åˆ™éšæœºæ•°ç”Ÿæˆå™¨ä½¿ç”¨çš„æ˜¯np.randomã€‚

æ¨¡å‹è®­ç»ƒç»“æŸåï¼Œå¯ä»¥ä½¿ç”¨ä¸‹åˆ—å‚æ•°ï¼šï¼š

- support_ï¼šæ”¯æŒå‘é‡çš„ä¸‹æ ‡ã€‚
- support_vectors_ï¼šæ”¯æŒå‘é‡ã€‚
- dual_coef_ï¼šæ”¯æŒå‘é‡çš„ç³»æ•°ã€‚
- coef_ï¼šçº¿æ€§æ ¸ä¸­çš„ç³»æ•°ã€‚
- intercept_ï¼šå†³ç­–å‡½æ•°ä¸­çš„å¸¸æ•°ã€‚
- classes_ï¼šåˆ†ç±»æ ‡ç­¾é›†åˆ

# ç¬¬7ç« è´å¶æ–¯æ–¹æ³•

```Python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®é›†
iris = load_iris()
X = iris.data
y = iris.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºé«˜æ–¯æœ´ç´ è´å¶æ–¯æ¨¡å‹
gnb = GaussianNB()

# è®­ç»ƒæ¨¡å‹
gnb.fit(X_train, y_train)

# é¢„æµ‹
predictions = gnb.predict(X_test)

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)
```

è´å¶æ–¯å‚æ•°ä¼˜åŒ–

å®‰è£…è´å¶æ–¯å‚æ•°ä¼˜åŒ–ï¼š pip install bayesian-optimization

> `bayesian-optimization`æ˜¯ä¸€ä¸ªåŸºäºè´å¶æ–¯æ¨ç†å’Œé«˜æ–¯è¿‡ç¨‹çš„çº¦æŸå…¨å±€ä¼˜åŒ–åŒ…ï¼Œå®ƒè¯•å›¾åœ¨å°½å¯èƒ½å°‘çš„è¿­ä»£ä¸­æ‰¾åˆ°æœªçŸ¥å‡½æ•°çš„æœ€å€¼ã€‚è´å¶æ–¯æœ€ä¼˜åŒ–èƒ½å¤Ÿåœ¨ä¸éœ€è¦å¤§é‡è®¡ç®—èµ„æºçš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆæ¢ç´¢å‚æ•°ç©ºé—´ã€‚

```Python
from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from bayes_opt import BayesianOptimization
import numpy as np

# åŠ è½½æ•°æ®é›†
iris = load_iris()
X, y = iris.data, iris.target

# å®šä¹‰ä¼˜åŒ–ç›®æ ‡å‡½æ•°
#å…ˆè¦å®šä¹‰ä¸€ä¸ªç›®æ ‡å‡½æ•°ã€‚æ¯”å¦‚æ­¤æ—¶ï¼Œå‡½æ•°è¾“å…¥ä¸ºéšæœºæ£®æ—çš„æ‰€æœ‰å‚æ•°ï¼Œ
#è¾“å‡ºä¸ºæ¨¡å‹äº¤å‰éªŒè¯5æ¬¡çš„AUCå‡å€¼ï¼Œä½œä¸ºæˆ‘ä»¬çš„ç›®æ ‡å‡½æ•°ã€‚
#å› ä¸ºbayes_optåº“åªæ”¯æŒæœ€å¤§å€¼ï¼Œæ‰€ä»¥æœ€åçš„è¾“å‡ºå¦‚æœæ˜¯è¶Šå°è¶Šå¥½ï¼Œé‚£ä¹ˆéœ€è¦åœ¨å‰é¢åŠ ä¸Šè´Ÿå·ï¼Œä»¥è½¬ä¸ºæœ€å¤§å€¼ã€‚
def decision_tree_cv(max_depth, min_samples_split):
    dt = DecisionTreeClassifier(max_depth=int(max_depth), min_samples_split=int(min_samples_split))
    scores = cross_val_score(dt, X, y, cv=5)
    return np.mean(scores)

# å®šä¹‰Bayesian Optimizationå¯¹è±¡
optimizer = BayesianOptimization(
    f=decision_tree_cv,
    pbounds={"max_depth": (1, 10), "min_samples_split": (2, 20)},
    random_state=42,
)

# æ‰§è¡Œä¼˜åŒ–è¿‡ç¨‹
optimizer.maximize(init_points=5, n_iter=25)

# è¾“å‡ºæœ€ä¼˜å‚æ•°
print("Best parameters found:")
print(optimizer.max["params"])

# ä½¿ç”¨æœ€ä¼˜å‚æ•°é‡æ–°è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°æ€§èƒ½
best_max_depth = int(optimizer.max["params"]["max_depth"])
best_min_samples_split = int(optimizer.max["params"]["min_samples_split"])
best_dt = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_split=best_min_samples_split)
best_dt.fit(X, y)
accuracy = best_dt.score(X, y)
print("Test accuracy with best parameters:", accuracy)
```

# ç¬¬8ç« é›†æˆå­¦ä¹ ä¸å®æˆ˜

**sklearnä¸­é›†æˆå­¦ä¹ æ–¹æ³•ä¸»è¦åŒ…æ‹¬Boostingã€Baggingå’ŒStackingç­‰æ–¹æ³•**ã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†ä»‹ç»ï¼š

1. **Boosting**ï¼šBoosting æ˜¯ä¸€ç§é€šè¿‡é€æ­¥è®­ç»ƒå¼±å­¦ä¹ å™¨å¹¶åŠ æƒç»„åˆæ¥æå‡æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ã€‚å…¶ä»£è¡¨ç®—æ³•åŒ…æ‹¬ AdaBoostã€Gradient Boosting ç­‰ã€‚AdaBoost é€šè¿‡è°ƒæ•´æ ·æœ¬æƒé‡æ¥èšç„¦äºä¹‹å‰è¢«é”™è¯¯åˆ†ç±»çš„æ ·æœ¬ï¼Œä»è€Œè®­ç»ƒå‡ºå¤šä¸ªå¼±å­¦ä¹ å™¨å¹¶è¿›è¡ŒåŠ æƒç»„åˆã€‚
2. **Bagging**ï¼šBagging é€šè¿‡è‡ªåŠ©é‡‡æ ·æ³•æ„å»ºå¤šä¸ªç‹¬ç«‹æ¨¡å‹ï¼Œç„¶åç»¼åˆè¿™äº›æ¨¡å‹çš„é¢„æµ‹ç»“æœæ¥æé«˜æ•´ä½“æ€§èƒ½ã€‚å¸¸è§çš„ Bagging æ–¹æ³•åŒ…æ‹¬éšæœºæ£®æ—ã€‚éšæœºæ£®æ—ä½¿ç”¨å¤šä¸ªå†³ç­–æ ‘å¯¹éšæœºæŠ½å–çš„ç‰¹å¾å­é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡æŠ•ç¥¨æˆ–å¹³å‡æ¥å¾—åˆ°æœ€ç»ˆé¢„æµ‹ç»“æœï¼Œä»è€Œé™ä½è¿‡æ‹Ÿåˆé£é™©å¹¶æé«˜æ¨¡å‹ç¨³å®šæ€§ã€‚
3. **Stacking**ï¼šStacking æ˜¯ä¸€ç§å¤šå±‚é›†æˆæ–¹æ³•ï¼Œå®ƒé€šè¿‡å°†å¤šä¸ªåŸºæ¨¡å‹çš„é¢„æµ‹ç»“æœä½œä¸ºè¾“å…¥ç‰¹å¾æ¥è®­ç»ƒä¸€ä¸ªé«˜å±‚æ¨¡å‹ï¼Œä»¥ç»¼åˆåŸºæ¨¡å‹çš„é¢„æµ‹ã€‚è¿™ç§æ–¹æ³•å¯ä»¥ç»“åˆä¸åŒç±»å‹å’Œæ€§è´¨çš„åŸºæ¨¡å‹ï¼Œä»è€Œæé«˜æ•´ä½“é¢„æµ‹æ€§èƒ½ã€‚

## Boosting

```Python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®é›†
iris = load_iris()
X, y = iris.data, iris.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºAdaBooståˆ†ç±»å™¨å®ä¾‹
ada_clf = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)

# è®­ç»ƒæ¨¡å‹
ada_clf.fit(X_train, y_train)

# é¢„æµ‹æµ‹è¯•é›†
y_pred = ada_clf.predict(X_test)

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
```

## Bagging

```Python
from sklearn.ensemble import BaggingClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®é›†
iris = load_iris()
X, y = iris.data, iris.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºBaggingåˆ†ç±»å™¨å®ä¾‹
bag_clf = BaggingClassifier(n_estimators=50, max_samples=0.8, bootstrap=True, random_state=42)

# è®­ç»ƒæ¨¡å‹
bag_clf.fit(X_train, y_train)

# é¢„æµ‹æµ‹è¯•é›†
y_pred = bag_clf.predict(X_test)

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
```

## éšæœºæ£®æ—

```Python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®é›†
iris = load_iris()
X = iris.data
y = iris.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºéšæœºæ£®æ—æ¨¡å‹
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# è®­ç»ƒæ¨¡å‹
rf.fit(X_train, y_train)

# é¢„æµ‹
predictions = rf.predict(X_test)

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)
```

## å¤šæ¨¡å‹èåˆ

```Python
from sklearn.ensemble import VotingClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# åŠ è½½æ•°æ®é›†
iris = load_iris()
X, y = iris.data, iris.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºåŸºå­¦ä¹ å™¨
clf1 = DecisionTreeClassifier(max_depth=4)
clf2 = LogisticRegression(solver='lbfgs', multi_class='multinomial')
clf3 = SVC(kernel='rbf', probability=True)

# åˆ›å»ºVotingåˆ†ç±»å™¨å®ä¾‹
voting_clf = VotingClassifier(estimators=[('dt', clf1), ('lr', clf2), ('svc', clf3)], voting='soft')

# è®­ç»ƒæ¨¡å‹
voting_clf.fit(X_train, y_train)

# é¢„æµ‹æµ‹è¯•é›†
y_pred = voting_clf.predict(X_test)

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
```

stackingç­–ç•¥

```Python
from sklearn.ensemble import StackingClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# åŠ è½½æ•°æ®é›†
iris = load_iris()
X, y = iris.data, iris.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºåŸºå­¦ä¹ å™¨
clf1 = DecisionTreeClassifier(max_depth=4)
clf2 = LogisticRegression(solver='lbfgs', multi_class='multinomial')
clf3 = SVC(kernel='rbf', probability=True)

# åˆ›å»ºStackingåˆ†ç±»å™¨å®ä¾‹
stacking_clf = StackingClassifier(estimators=[('dt', clf1), ('lr', clf2), ('svc', clf3)], final_estimator=LogisticRegression())

# è®­ç»ƒæ¨¡å‹
stacking_clf.fit(X_train, y_train)

# é¢„æµ‹æµ‹è¯•é›†
y_pred = stacking_clf.predict(X_test)

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
```

## æœºå™¨å­¦ä¹ å®æˆ˜

è´·æ¬¾è¿çº¦é¢„æµ‹[é›¶åŸºç¡€å…¥é—¨é‡‘èé£æ§-è´·æ¬¾è¿çº¦é¢„æµ‹_å­¦ä¹ èµ›_èµ›é¢˜ä¸æ•°æ®_å¤©æ± å¤§èµ›](https://tianchi.aliyun.com/competition/entrance/531830/information)

å‚è€ƒä»£ç ï¼šhttps://github.com/datawhalechina/machine-learning-toy-code/blob/main/%E5%A4%A9%E6%B1%A0%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7.ipynb

